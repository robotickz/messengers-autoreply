import OpenAI from 'openai';
import { Message } from '../models';
import { convertAudioInMemory } from '../utils/oggToMP3';
import dotenv from 'dotenv';
import { getChatById, getFileToken, getFileUrl, getMediaRecord, saveChat } from "../storage/pocketbase";
dotenv.config();

const OPENAI_API_KEY = process.env.OPENAI_API_KEY;

if (!OPENAI_API_KEY) {
  throw new Error('OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è');
}

const client = new OpenAI({
  apiKey: OPENAI_API_KEY,
});

export async function getAIResponse(message: Message, threadId?: string): Promise<string> {
  try {

    if (message.type === "voice" || message.type === "audio") {
      console.log("Processing audio message");
      return await handleAudioMessage(message);
    } else if (message.type === "image") {
      console.log("Processing image message");
      return await handleImageMessage(message);
    }

    return await handleTextMessage(message, threadId!);
  } catch (error) {
    console.error("OpenAI Assistants API error:", error);
    return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞.";
  }
}

async function handleTextMessage(message: Message, openAIThreadId?: string): Promise<string> {
  const assistantId = process.env.OPENAI_ASSISTANT_ID;

  if (!assistantId) {
    throw new Error("OPENAI_ASSISTANT_ID is not defined in environment");
  }

  let threadId = openAIThreadId!;

  if (!threadId) {
    const thread = await client.beta.threads.create();
    threadId = thread.id;
    let chat = await getChatById(message.chatId);
    chat!.openAIThreadId = threadId;
    await saveChat(chat!);
    console.log(`Created new thread ${threadId} for chat ${chat!.id}`);
  }

  await client.beta.threads.messages.create(threadId, {
    role: "user",
    content: message.content
  });

  const run = await client.beta.threads.runs.create(threadId, {
    assistant_id: assistantId
  });

  let runStatus = await client.beta.threads.runs.retrieve(threadId, run.id);

  const startTime = Date.now();
  const timeout = 30000;

  while (runStatus.status !== "completed") {
    if (["failed", "cancelled", "expired"].includes(runStatus.status)) {
      throw new Error(`Run ${run.id} failed with status ${runStatus.status}`);
    }

    if (Date.now() - startTime > timeout) {
      throw new Error("Response timed out after 30 seconds");
    }

    await new Promise(resolve => setTimeout(resolve, 1000));
    runStatus = await client.beta.threads.runs.retrieve(threadId, run.id);
  }

  const messages = await client.beta.threads.messages.list(threadId, {
    order: "desc",
    limit: 1
  });

  if (messages.data.length === 0 || messages.data[0].role !== "assistant") {
    throw new Error("No assistant response found");
  }

  const content = messages.data[0].content[0];
  if (content.type !== "text") {
    throw new Error(`Unexpected content type: ${content.type}`);
  }

  return content.text.value;
}

async function handleImageMessage(message: Message): Promise<string> {
  try {
    if (!message.mediaFileId) {
      return "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.";
    }

    try {
      const fileToken = await getFileToken();
      const mediaRecord = await getMediaRecord(message.mediaFileId);
      const url = await getFileUrl(mediaRecord, mediaRecord.file, { 'token': fileToken });
      const response = await fetch(url);

      if (!response.ok) {
        console.log(`Download failed: ${response.status} ${response.statusText}`);
        return "–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ —Ñ–∞–π–ª—É.";
      }

      const contentType = response.headers.get('content-type');
      const arrayBuffer = await response.arrayBuffer();
      const base64Image = Buffer.from(arrayBuffer).toString('base64');
      const dataUrl = `data:${contentType || 'image/jpeg'};base64,${base64Image}`;

      const visionResponse = await client.chat.completions.create({
        model: process.env.OPENAI_VISION_MODEL!,
        messages: [
          {
            role: "user",
            content: [
              {
                type: "text",
                text: message.content || "–ß—Ç–æ –Ω–∞ —ç—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏?"
              },
              {
                type: "image_url",
                image_url: {
                  url: dataUrl
                }
              }
            ]
          }
        ],
        max_tokens: 500
      });

      const imageDescription = visionResponse.choices[0].message.content || "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.";

      const contextualContent = message.content
        ? `${message.content}\n\n[–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: ${imageDescription}]`
        : `[–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: ${imageDescription}]`;

      const textMessage: Message = {
        ...message,
        content: contextualContent,
        type: "text"
      };

      const assistantResponse = await handleTextMessage(textMessage);

      return `üì∑ –û–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: "${imageDescription}"\n\nü§ñ –û—Ç–≤–µ—Ç: ${assistantResponse}`;

    } catch (error) {
      console.error("Error getting image from PocketBase:", error);
      return "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.";
    }
  } catch (error) {
    console.error("Error in handleImageMessage:", error);
    throw error;
  }
}

async function handleAudioMessage(message: Message): Promise<string> {
  try {
    if (!message.mediaFileId) {
      return "–ê—É–¥–∏–æ—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω.";
    }

    try {
      const fileToken = await getFileToken()
      const mediaRecord = await getMediaRecord(message.mediaFileId);
      const url = await getFileUrl(mediaRecord, mediaRecord.file, { 'token': fileToken });

      const response = await fetch(url);
      if (!response.ok) {
        console.log(`Download failed: ${response.status} ${response.statusText}`);
        return "–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –∞—É–¥–∏–æ—Ñ–∞–π–ª—É.";
      }

      const originalBuffer = Buffer.from(await response.arrayBuffer());
      const formData = new FormData();
      let model = process.env.OPENAI_AUDIO_MODEL! ?? 'whisper-1';
      formData.append('model', model);
      let audioBlob = new Blob([originalBuffer], { type: 'audio/mp4' });

      if (message.source === "telegram") {
        const mp3Buffer = await convertAudioInMemory(originalBuffer);
        audioBlob = new Blob([mp3Buffer], { type: 'audio/mp3' });
        formData.append('file', audioBlob, 'audio.mp3');
      } else {
        formData.append('file', audioBlob, 'audio.mp4');
      }

      const transcriptionResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`
        },
        body: formData
      });

      if (!transcriptionResponse.ok) {
        const errorText = await transcriptionResponse.text();
        console.error("Transcription error:", errorText);
        return "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å –∞—É–¥–∏–æ.";
      }

      const transcriptionResult = await transcriptionResponse.json();
      const transcribedText = transcriptionResult.text;

      if (!transcribedText || transcribedText.trim() === "") {
        return "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ –∞—É–¥–∏–æ—Å–æ–æ–±—â–µ–Ω–∏–∏.";
      }

      const textMessage = {
        ...message,
        content: transcribedText,
        type: "text"
      } as Message;

      const assistantResponse = await handleTextMessage(textMessage);
      return `üìù –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: "${transcribedText}"\n\nü§ñ –û—Ç–≤–µ—Ç: ${assistantResponse}`;

    } catch (error) {
      console.error("Error processing audio from PocketBase:", error);
      return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∞—É–¥–∏–æ—Å–æ–æ–±—â–µ–Ω–∏—è.";
    }
  } catch (error) {
    console.error("Error in handleAudioMessage:", error);
    throw error;
  }
}